#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å›ºå®šç‚¹äº‘å¯¹é…å‡†æµ‹è¯•è„šæœ¬
Fixed Point Cloud Pair Registration Test Script

è¯¥è„šæœ¬ç”¨äºæµ‹è¯•å›ºå®šçš„ç‚¹äº‘å¯¹åœ¨ç»™å®šæ‰°åŠ¨ä¸‹çš„é…å‡†æ•ˆæœ
This script is used to test the registration effect of fixed point cloud pairs under given perturbations
"""

import argparse
import torch
import numpy as np
import sys
import os
import time
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
# Add project root directory to Python path
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))

def options():
    """å‘½ä»¤è¡Œå‚æ•°è§£æ"""
    parser = argparse.ArgumentParser(description='å›ºå®šç‚¹äº‘å¯¹é…å‡†æµ‹è¯• - Fixed Point Cloud Pair Registration Test')
    
    # å¿…éœ€å‚æ•°
    parser.add_argument('--source-cloud', required=True, type=str,
                        help='æºç‚¹äº‘æ–‡ä»¶è·¯å¾„ Source point cloud file path')
    parser.add_argument('--target-cloud', required=True, type=str,
                        help='ç›®æ ‡ç‚¹äº‘æ–‡ä»¶è·¯å¾„ Target point cloud file path')
    parser.add_argument('--model-path', required=True, type=str,
                        help='æ¨¡å‹æ–‡ä»¶è·¯å¾„ Model file path')
    parser.add_argument('--perturbation', required=True, type=str,
                        help='æ‰°åŠ¨å€¼ï¼Œé€—å·åˆ†éš”(rx,ry,rz,tx,ty,tz) Perturbation values, comma-separated')
    
    # è¾“å‡ºè®¾ç½®
    parser.add_argument('--output-csv', default=None, type=str,
                        help='è¾“å‡ºCSVæ–‡ä»¶è·¯å¾„ Output CSV file path')
    parser.add_argument('--output-dir', default='./results/fixed_pair_test', type=str,
                        help='è¾“å‡ºç›®å½• Output directory')
    
    # æ¨¡å‹é…ç½®
    parser.add_argument('--model-type', default='mamba3d', choices=['pointnet', 'attention', 'mamba3d', 'mamba3d_v2', 'fast_attention', 'cformer'],
                        help='æ¨¡å‹ç±»å‹ Model type')
    parser.add_argument('--dim-k', default=1024, type=int,
                        help='ç‰¹å¾ç»´åº¦ Feature dimension')
    parser.add_argument('--num-mamba-blocks', default=1, type=int,
                        help='Mambaå—æ•°é‡ Number of Mamba blocks')
    parser.add_argument('--d-state', default=8, type=int,
                        help='çŠ¶æ€ç©ºé—´ç»´åº¦ State space dimension')
    parser.add_argument('--expand', default=2, type=float,
                        help='æ‰©å±•å› å­ Expansion factor')
    parser.add_argument('--symfn', default='max', choices=['max', 'avg'],
                        help='å¯¹ç§°å‡½æ•° Symmetric function')
    
    # ç®—æ³•å‚æ•°
    parser.add_argument('--max-iter', default=20, type=int,
                        help='LKæœ€å¤§è¿­ä»£æ¬¡æ•° Maximum LK iterations')
    parser.add_argument('--delta', default=1.0e-4, type=float,
                        help='LKæ­¥é•¿ LK step size')
    parser.add_argument('--num-points', default=1024, type=int,
                        help='ç‚¹äº‘ç‚¹æ•° Number of points in point cloud')
    
    # è®¾å¤‡è®¾ç½®
    parser.add_argument('--device', default='cuda:0', type=str,
                        help='è®¡ç®—è®¾å¤‡ Computing device')
    
    # ä½“ç´ åŒ–å‚æ•°
    parser.add_argument('--use-voxelization', action='store_true', default=False,
                        help='å¯ç”¨ä½“ç´ åŒ– Enable voxelization')
    parser.add_argument('--voxel-size', default=4, type=float,
                        help='ä½“ç´ å¤§å° Voxel size')
    parser.add_argument('--voxel-grid-size', default=32, type=int,
                        help='ä½“ç´ ç½‘æ ¼å°ºå¯¸ Voxel grid size')
    parser.add_argument('--max-voxel-points', default=100, type=int,
                        help='æ¯ä¸ªä½“ç´ æœ€å¤§ç‚¹æ•° Maximum points per voxel')
    parser.add_argument('--max-voxels', default=20000, type=int,
                        help='æœ€å¤§ä½“ç´ æ•°é‡ Maximum number of voxels')
    parser.add_argument('--min-voxel-points-ratio', default=0.1, type=float,
                        help='æœ€å°ä½“ç´ ç‚¹æ•°æ¯”ä¾‹ Minimum voxel points ratio')
    
    # è°ƒè¯•é€‰é¡¹
    parser.add_argument('--verbose', action='store_true', default=False,
                        help='è¯¦ç»†è¾“å‡º Verbose output')
    parser.add_argument('--save-clouds', action='store_true', default=False,
                        help='ä¿å­˜å¤„ç†åçš„ç‚¹äº‘ Save processed point clouds')
    
    return parser.parse_args()

def load_model(args):
    """åŠ è½½æ¨¡å‹"""
    from ptlk import pointlk
    from ptlk.data.datasets import VoxelizationConfig
    
    print(f"æ­£åœ¨åŠ è½½æ¨¡å‹: {args.model_path}")
    
    # åˆ›å»ºä½“ç´ åŒ–é…ç½®
    voxel_config = VoxelizationConfig(
        voxel_size=args.voxel_size,
        voxel_grid_size=args.voxel_grid_size,
        max_voxel_points=args.max_voxel_points,
        max_voxels=args.max_voxels,
        min_voxel_points_ratio=args.min_voxel_points_ratio
    )
    
    # æ ¹æ®æ¨¡å‹ç±»å‹åˆ›å»ºæ¨¡å‹
    if args.model_type == 'pointnet':
        from ptlk.PointNet_files.pointnet_original import PointNet as FeatureModel
        ptnet = FeatureModel(args.dim_k)
    elif args.model_type == 'mamba3d':
        from ptlk.data.mamba3d import Mamba3D
        ptnet = Mamba3D(
            dim_k=args.dim_k,
            num_blocks=args.num_mamba_blocks,
            d_state=args.d_state,
            expand=args.expand,
            symfn=args.symfn
        )
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {args.model_type}")
    
    # åˆ›å»ºPointNetLKæ¨¡å‹
    model = pointlk.PointNetLK(
        feature_model=ptnet,
        delta=args.delta,
        xtol=1.0e-7,
        p0_zero_mean=True,
        p1_zero_mean=True,
        pooling='max',
        use_voxelization=args.use_voxelization,
        voxel_config=voxel_config
    )
    
    # åŠ è½½é¢„è®­ç»ƒæƒé‡
    if os.path.exists(args.model_path):
        checkpoint = torch.load(args.model_path, map_location='cpu')
        if 'model_state_dict' in checkpoint:
            model.load_state_dict(checkpoint['model_state_dict'])
        else:
            model.load_state_dict(checkpoint)
        print("âœ… æˆåŠŸåŠ è½½æ¨¡å‹æƒé‡")
    else:
        raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {args.model_path}")
    
    # ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡
    device = torch.device(args.device)
    model = model.to(device)
    model.eval()
    
    return model, device

def load_point_cloud(file_path, num_points=1024):
    """åŠ è½½ç‚¹äº‘æ–‡ä»¶"""
    import plyfile
    
    print(f"æ­£åœ¨åŠ è½½ç‚¹äº‘: {file_path}")
    
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"ç‚¹äº‘æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
    
    # è¯»å–PLYæ–‡ä»¶
    plydata = plyfile.PlyData.read(file_path)
    vertices = plydata['vertex']
    
    # æå–XYZåæ ‡
    points = np.column_stack([
        vertices['x'].astype(np.float32),
        vertices['y'].astype(np.float32), 
        vertices['z'].astype(np.float32)
    ])
    
    print(f"åŸå§‹ç‚¹äº‘å½¢çŠ¶: {points.shape}")
    
    # é‡é‡‡æ ·åˆ°æŒ‡å®šç‚¹æ•°
    if len(points) > num_points:
        # éšæœºé‡‡æ ·
        indices = np.random.choice(len(points), num_points, replace=False)
        points = points[indices]
    elif len(points) < num_points:
        # é‡å¤é‡‡æ ·
        indices = np.random.choice(len(points), num_points, replace=True)
        points = points[indices]
    
    print(f"é‡é‡‡æ ·åç‚¹äº‘å½¢çŠ¶: {points.shape}")
    
    return points

def create_perturbation_matrix(perturbation_values):
    """åˆ›å»ºæ‰°åŠ¨å˜æ¢çŸ©é˜µ"""
    import ptlk.se3 as se3
    
    # è§£ææ‰°åŠ¨å€¼ (rx,ry,rz,tx,ty,tz)
    rx, ry, rz, tx, ty, tz = perturbation_values
    
    # åˆ›å»ºæ—‹è½¬å’Œå¹³ç§»
    rotation = se3.euler_to_so3(rx, ry, rz)  # æ¬§æ‹‰è§’è½¬æ—‹è½¬çŸ©é˜µ
    translation = np.array([tx, ty, tz])
    
    # æ„å»º4x4å˜æ¢çŸ©é˜µ
    transform_matrix = np.eye(4)
    transform_matrix[:3, :3] = rotation
    transform_matrix[:3, 3] = translation
    
    return transform_matrix

def apply_perturbation(points, perturbation_matrix):
    """å¯¹ç‚¹äº‘åº”ç”¨æ‰°åŠ¨"""
    # è½¬æ¢ä¸ºé½æ¬¡åæ ‡
    points_homo = np.column_stack([points, np.ones(len(points))])
    
    # åº”ç”¨å˜æ¢
    transformed_points = (perturbation_matrix @ points_homo.T).T
    
    # è¿”å›3Dåæ ‡
    return transformed_points[:, :3]

def compute_registration_error(gt_transform, pred_transform):
    """è®¡ç®—é…å‡†è¯¯å·®"""
    import ptlk.se3 as se3
    
    # è®¡ç®—ç›¸å¯¹å˜æ¢
    relative_transform = np.linalg.inv(gt_transform) @ pred_transform
    
    # åˆ†è§£ä¸ºæ—‹è½¬å’Œå¹³ç§»
    rotation_part = relative_transform[:3, :3]
    translation_part = relative_transform[:3, 3]
    
    # è®¡ç®—æ—‹è½¬è¯¯å·® (è§’åº¦)
    rotation_error = np.arccos(np.clip((np.trace(rotation_part) - 1) / 2, -1, 1))
    rotation_error_deg = np.degrees(rotation_error)
    
    # è®¡ç®—å¹³ç§»è¯¯å·® (æ¬§å‡ é‡Œå¾—è·ç¦»)
    translation_error = np.linalg.norm(translation_part)
    
    return rotation_error_deg, translation_error

def main():
    """ä¸»å‡½æ•°"""
    args = options()
    
    print("========== å›ºå®šç‚¹äº‘å¯¹é…å‡†æµ‹è¯• ==========")
    print(f"æºç‚¹äº‘: {args.source_cloud}")
    print(f"ç›®æ ‡ç‚¹äº‘: {args.target_cloud}")
    print(f"æ¨¡å‹: {args.model_path}")
    print(f"æ‰°åŠ¨: {args.perturbation}")
    print(f"è®¾å¤‡: {args.device}")
    print("")
    
    # è§£ææ‰°åŠ¨å€¼
    try:
        perturbation_values = [float(x.strip()) for x in args.perturbation.split(',')]
        if len(perturbation_values) != 6:
            raise ValueError(f"æ‰°åŠ¨å€¼å¿…é¡»æ˜¯6ä¸ªæ•°å­—ï¼Œå½“å‰æä¾›äº†{len(perturbation_values)}ä¸ª")
        print(f"è§£æçš„æ‰°åŠ¨å€¼: {perturbation_values}")
    except Exception as e:
        print(f"âŒ æ‰°åŠ¨å€¼è§£æé”™è¯¯: {e}")
        return
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(args.output_dir, exist_ok=True)
    
    try:
        # åŠ è½½æ¨¡å‹
        print("\n1. åŠ è½½æ¨¡å‹...")
        model, device = load_model(args)
        
        # åŠ è½½ç‚¹äº‘
        print("\n2. åŠ è½½ç‚¹äº‘...")
        source_points = load_point_cloud(args.source_cloud, args.num_points)
        target_points = load_point_cloud(args.target_cloud, args.num_points)
        
        # åˆ›å»ºæ‰°åŠ¨çŸ©é˜µ
        print("\n3. åˆ›å»ºæ‰°åŠ¨...")
        perturbation_matrix = create_perturbation_matrix(perturbation_values)
        print(f"æ‰°åŠ¨çŸ©é˜µ:\n{perturbation_matrix}")
        
        # å¯¹æºç‚¹äº‘åº”ç”¨æ‰°åŠ¨
        source_points_perturbed = apply_perturbation(source_points, perturbation_matrix)
        
        # å‡†å¤‡æ•°æ®
        template = torch.from_numpy(target_points.T).float().unsqueeze(0).to(device)  # [1, 3, N]
        source = torch.from_numpy(source_points_perturbed.T).float().unsqueeze(0).to(device)  # [1, 3, N]
        
        print(f"æ¨¡æ¿ç‚¹äº‘å½¢çŠ¶: {template.shape}")
        print(f"æºç‚¹äº‘å½¢çŠ¶: {source.shape}")
        
        # æ‰§è¡Œé…å‡†
        print("\n4. æ‰§è¡Œé…å‡†...")
        start_time = time.time()
        
        with torch.no_grad():
            result = model(template, source, maxiter=args.max_iter)
            
        end_time = time.time()
        registration_time = end_time - start_time
        print(f"é…å‡†è€—æ—¶: {registration_time:.4f} ç§’")
        
        # æå–ç»“æœ
        if isinstance(result, tuple):
            predicted_transform = result[0]
        else:
            predicted_transform = result
        
        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        predicted_transform_np = predicted_transform.cpu().numpy().squeeze()
        
        print(f"é¢„æµ‹å˜æ¢çŸ©é˜µå½¢çŠ¶: {predicted_transform_np.shape}")
        print(f"é¢„æµ‹å˜æ¢çŸ©é˜µ:\n{predicted_transform_np}")
        
        # è®¡ç®—é…å‡†è¯¯å·®
        print("\n5. è®¡ç®—é…å‡†è¯¯å·®...")
        gt_transform = np.linalg.inv(perturbation_matrix)  # çœŸå®å˜æ¢æ˜¯æ‰°åŠ¨çš„é€†
        rotation_error, translation_error = compute_registration_error(gt_transform, predicted_transform_np)
        
        print(f"æ—‹è½¬è¯¯å·®: {rotation_error:.6f} åº¦")
        print(f"å¹³ç§»è¯¯å·®: {translation_error:.6f} mm")
        
        # ä¿å­˜ç»“æœ
        print("\n6. ä¿å­˜ç»“æœ...")
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        
        # ä¿å­˜åˆ°CSV
        if args.output_csv:
            output_csv = args.output_csv
        else:
            output_csv = os.path.join(args.output_dir, f"fixed_pair_result_{timestamp}.csv")
        
        # å†™å…¥CSVæ–‡ä»¶
        with open(output_csv, 'w') as f:
            f.write("source_cloud,target_cloud,perturbation,rotation_error_deg,translation_error_mm,registration_time_sec\n")
            f.write(f"{os.path.basename(args.source_cloud)},{os.path.basename(args.target_cloud)},{args.perturbation},{rotation_error:.6f},{translation_error:.6f},{registration_time:.4f}\n")
        
        print(f"âœ… ç»“æœå·²ä¿å­˜åˆ°: {output_csv}")
        
        # ä¿å­˜è¯¦ç»†ä¿¡æ¯
        detail_file = os.path.join(args.output_dir, f"fixed_pair_detail_{timestamp}.txt")
        with open(detail_file, 'w') as f:
            f.write("=== å›ºå®šç‚¹äº‘å¯¹é…å‡†æµ‹è¯•è¯¦ç»†ç»“æœ ===\n")
            f.write(f"æµ‹è¯•æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"æºç‚¹äº‘: {args.source_cloud}\n")
            f.write(f"ç›®æ ‡ç‚¹äº‘: {args.target_cloud}\n")
            f.write(f"æ¨¡å‹: {args.model_path}\n")
            f.write(f"æ¨¡å‹ç±»å‹: {args.model_type}\n")
            f.write(f"è®¾å¤‡: {args.device}\n")
            f.write(f"ç‚¹äº‘ç‚¹æ•°: {args.num_points}\n")
            f.write(f"æœ€å¤§è¿­ä»£: {args.max_iter}\n")
            f.write(f"æ‰°åŠ¨å€¼: {perturbation_values}\n")
            f.write(f"æ‰°åŠ¨çŸ©é˜µ:\n{perturbation_matrix}\n")
            f.write(f"é¢„æµ‹å˜æ¢çŸ©é˜µ:\n{predicted_transform_np}\n")
            f.write(f"æ—‹è½¬è¯¯å·®: {rotation_error:.6f} åº¦\n")
            f.write(f"å¹³ç§»è¯¯å·®: {translation_error:.6f} mm\n")
            f.write(f"é…å‡†è€—æ—¶: {registration_time:.4f} ç§’\n")
        
        print(f"âœ… è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {detail_file}")
        
        # å¯é€‰ï¼šä¿å­˜ç‚¹äº‘
        if args.save_clouds:
            print("\n7. ä¿å­˜ç‚¹äº‘...")
            cloud_dir = os.path.join(args.output_dir, f"clouds_{timestamp}")
            os.makedirs(cloud_dir, exist_ok=True)
            
            np.savetxt(os.path.join(cloud_dir, "source_original.txt"), source_points)
            np.savetxt(os.path.join(cloud_dir, "source_perturbed.txt"), source_points_perturbed)
            np.savetxt(os.path.join(cloud_dir, "target.txt"), target_points)
            
            print(f"âœ… ç‚¹äº‘å·²ä¿å­˜åˆ°: {cloud_dir}")
        
        print("\nğŸ‰ æµ‹è¯•å®Œæˆ!")
        print(f"ğŸ“Š æ—‹è½¬è¯¯å·®: {rotation_error:.6f} åº¦")
        print(f"ğŸ“Š å¹³ç§»è¯¯å·®: {translation_error:.6f} mm")
        print(f"â±ï¸  é…å‡†è€—æ—¶: {registration_time:.4f} ç§’")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return 1
    
    return 0

if __name__ == '__main__':
    sys.exit(main()) 